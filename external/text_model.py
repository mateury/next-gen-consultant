from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
import os
import re
import json
from .mcp_server import (
    check_customer,
    get_product_catalog
)

SYSTEM_PROMPT = """
JesteÅ› wirtualnym konsultantem Play - profesjonalnym doradcÄ… ds. sprzedaÅ¼y usÅ‚ug telekomunikacyjnych.

Twoja rola:
- PomÃ³c klientowi wybraÄ‡ najlepszÄ… ofertÄ™ (internet, TV, telefon komÃ³rkowy)
- WyjaÅ›niÄ‡ szczegÃ³Å‚y produktÃ³w i promocji
- SprawdziÄ‡ obecne usÅ‚ugi klienta po numerze PESEL
- ZaÅ‚oÅ¼yÄ‡ zamÃ³wienie w systemie
- OdpowiadaÄ‡ na pytania o status zamÃ³wieÅ„ i usÅ‚ug

DOSTÄ˜PNE NARZÄ˜DZIA MCP:
MoÅ¼esz uÅ¼ywaÄ‡ nastÄ™pujÄ…cych narzÄ™dzi poprzez specjalnÄ… skÅ‚adniÄ™:

1. [CHECK_CUSTOMER: pesel]
   PrzykÅ‚ad: [CHECK_CUSTOMER: 85010112345]
   Sprawdza dane klienta i jego usÅ‚ugi po PESEL

2. [GET_CATALOG: typ]
   PrzykÅ‚ad: [GET_CATALOG: MOBILE] lub [GET_CATALOG: ALL]
   Pobiera katalog produktÃ³w (MOBILE, INTERNET, TV lub ALL dla wszystkich)

Zasady:
- BÄ…dÅº uprzejmy i profesjonalny
- Zadawaj pytania, aby zrozumieÄ‡ potrzeby klienta
- JeÅ›li klient podaje PESEL, ZAWSZE najpierw sprawdÅº jego usÅ‚ugi uÅ¼ywajÄ…c [CHECK_CUSTOMER: pesel]
- Pokazuj katalog produktÃ³w uÅ¼ywajÄ…c [GET_CATALOG: typ]
- Kalkuluj cenÄ™ przed utworzeniem zamÃ³wienia uÅ¼ywajÄ…c [CALCULATE_PRICE: ids]
- Zawsze potwierdzaj wszystkie dane przed utworzeniem zamÃ³wienia
- UÅ¼ywaj emoji dla lepszej komunikacji ðŸ˜Š

Workflow:
1. Zapytaj o potrzeby klienta (internet, TV, telefon?)
2. JeÅ›li klient podaje PESEL - uÅ¼yj [CHECK_CUSTOMER: pesel]
3. UÅ¼yj [GET_CATALOG: typ] aby pokazaÄ‡ oferty
4. PomÃ³Å¼ wybraÄ‡ odpowiednie pakiety

WAÅ»NE: 
- Formatuj odpowiedzi czytelnie
- UÅ¼ywaj narzÄ™dzi gdy to konieczne (zapisuj je DOKÅADNIE jak w przykÅ‚adach)
- Nigdy nie wymyÅ›laj danych - zawsze pytaj klienta
- Po wykonaniu narzÄ™dzia, przeanalizuj wynik i odpowiedz klientowi
"""


class ModelConnector:
    def __init__(self, system_prompt=SYSTEM_PROMPT):
        self.llm = ChatOpenAI(
            base_url="https://api.scaleway.ai/2d6e7638-f7f5-41f4-b61c-79209c1785be/v1",
            api_key=os.environ.get("SCW_SECRET_KEY"),
            model="gpt-oss-120b",
            max_tokens=2048,
            temperature=0.7,
            top_p=1,
            presence_penalty=0,
            streaming=True
        )
        
        self.parser = StrOutputParser()
        self.conversation_history = [SystemMessage(content=system_prompt)]
    
    async def _execute_tool_command(self, command: str) -> str:
        """Wykonuje komendÄ™ narzÄ™dzia MCP"""
        try:
            # [CHECK_CUSTOMER: pesel]
            if command.startswith("[CHECK_CUSTOMER:"):
                pesel = command[16:-1].strip()
                return await check_customer(pesel)
            
            # [GET_CATALOG: type]
            elif command.startswith("[GET_CATALOG:"):
                catalog_type = command[13:-1].strip()
                if catalog_type.upper() == "ALL":
                    catalog_type = None
                return await get_product_catalog(catalog_type)
            
            else:
                return f"âŒ Nieznana komenda: {command}"
                
        except json.JSONDecodeError as e:
            return f"âŒ BÅ‚Ä…d parsowania JSON: {str(e)}"
        except Exception as e:
            return f"âŒ BÅ‚Ä…d wykonania narzÄ™dzia: {str(e)}"
    
    async def _process_response_with_tools(self, text: str, stream_callback=None) -> str:
        """Przetwarza odpowiedÅº i wykonuje narzÄ™dzia MCP jeÅ›li sÄ… w tekÅ›cie"""
        
        # Szukaj komend narzÄ™dzi w odpowiedzi
        tool_pattern = r'\[(CHECK_CUSTOMER|GET_CATALOG):[^\]]+\]'
        tools_found = re.findall(tool_pattern, text, re.IGNORECASE)
        
        if not tools_found:
            return text
        
        # ZnajdÅº peÅ‚ne komendy
        full_commands = re.findall(r'\[(?:CHECK_CUSTOMER|GET_CATALOG):[^\]]+\]', text, re.IGNORECASE)
        
        # Wykonaj kaÅ¼de narzÄ™dzie
        results = []
        for tool_command in full_commands:
            if stream_callback:
                await stream_callback(f"\n\nðŸ”§ WykonujÄ™: {tool_command}\n\n")
            
            result = await self._execute_tool_command(tool_command)
            results.append((tool_command, result))
            
            # Stream rezultatu narzÄ™dzia
            if stream_callback:
                await stream_callback(result)
                await stream_callback("\n\n")
        
        # Dodaj rezultaty do historii i poproÅ› o finalnÄ… odpowiedÅº
        tool_results_text = "\n\n".join([
            f"WYNIK NARZÄ˜DZIA {cmd}:\n{res}" 
            for cmd, res in results
        ])
        
        self.conversation_history.append(
            HumanMessage(content=f"TOOL_RESULTS:\n{tool_results_text}\n\nNa podstawie powyÅ¼szych wynikÃ³w, sformuÅ‚uj pomocnÄ… odpowiedÅº dla klienta. NIE uÅ¼ywaj juÅ¼ wiÄ™cej narzÄ™dzi, tylko przeanalizuj wyniki.")
        )
        
        # Pobierz finalnÄ… odpowiedÅº
        final_response = ''
        async for chunk in self.llm.astream(self.conversation_history):
            if chunk.content:
                final_response += chunk.content
                if stream_callback:
                    await stream_callback(chunk.content)
        
        return final_response
    
    async def get_model_response(self, input_text: str, stream_callback=None) -> str:
        """
        Get response from the model with MCP tool support.
        
        Args:
            input_text: User's message
            stream_callback: Optional callback for streaming (receives chunks)
        
        Returns:
            Complete response text
        """
        # Add user message to history
        self.conversation_history.append(HumanMessage(content=input_text))
        
        output_text = ''
        
        # Get initial response
        async for chunk in self.llm.astream(self.conversation_history):
            if chunk.content:
                output_text += chunk.content
                if stream_callback:
                    await stream_callback(chunk.content)
        
        # Check if response contains tool commands
        tool_pattern = r'\[(?:CHECK_CUSTOMER|GET_CATALOG):[^\]]+\]'
        tools_found = re.findall(tool_pattern, output_text, re.IGNORECASE)
        
        if tools_found:
            # Add AI response to history
            self.conversation_history.append(AIMessage(content=output_text))
            
            # Execute tools and get final response
            final_text = await self._process_response_with_tools(output_text, stream_callback)
            
            # Add final response to history
            self.conversation_history.append(AIMessage(content=final_text))
            
            return final_text
        else:
            # No tools, just add response to history
            self.conversation_history.append(AIMessage(content=output_text))
            return output_text